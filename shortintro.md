---
layout: default
permalink: /shortintro
nav_order: 1
title: What is COCO?
---

# A Short Introduction to COCO  #

COCO (COmparing Continuous Optimizers) is a platform for systematic and sound comparisons of real-parameter global optimizers. 
COCO provides benchmark function testbeds, experimentation templates which are easy to parallelize, and tools for processing and 
visualizing data generated by one or several optimizers. The COCO platform has been used for the
[Black-Box-Optimization-Benchmarking (BBOB) workshops](http://numbbo.github.io/workshops/) that took place during the GECCO
conference in 2009, 2010, 2012, 2013, 2015-2019, and in 2021. It was also used at the IEEE Congress on Evolutionary Computation
(CEC'2015) in Sendai, Japan.

The COCO experiment source code has been rewritten in the years 2014-2015 and the current production code is available on our
[COCO Github page](https://github.com/numbbo/coco). The old code is still available [here](oldcode/bboball15.03.tar.gz)
 and shall be used for experiments on the noisy test suite until this test suite will be available in the new code as well. 

For a general introduction to the COCO software and its underlying concepts of performance assessment, please see this article 

> [Hansen, N., Auger, A., Ros, R., Mersmann, O., Tu≈°ar, T., & Brockhoff, D. (2021).](https://www.tandfonline.com/doi/abs/10.1080/10556788.2020.1808977) [COCO: A platform for comparing continuous optimizers in a black-box setting.](https://www.tandfonline.com/doi/abs/10.1080/10556788.2020.1808977) [_Optimization Methods and Software, 36_(1), 114-144.](https://www.tandfonline.com/doi/abs/10.1080/10556788.2020.1808977)

or its publicly available version [on HAL](https://hal.inria.fr/hal-01294124v4/document).



<link rel="stylesheet" href="{{ '/assets/css/custom.css' | relative_url }}"/>
