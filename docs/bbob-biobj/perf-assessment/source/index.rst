
Biobjective Performance Assessment with the Coco Platform
=========================================================

This document details the specificities when assessing the performance of numerical black-box optimizers
on multi-objective problems within the Coco platform and in particular on the biobjective test suite
``bbob-biobj``, described in more detail in [bbob-biobj-functions-doc]_ .

Opposed to the single-objective ``bbob`` test suite, the biobjective ``bbob-biobj`` test suite does
not provide analytical forms of its optima, i.e. the Pareto set/Pareto front except for one of
its 55 functions. The performance assessment therefore has to be relative to the best known
approximations and this document details how this is implemented.



.. todo::   * perf assessement is relative - we face a problem: we do not have the optimum.
			* How do we deal with this problem? [ this should probably be a section]
				* estimate the optimum
				* but approximation, meant to change / be improved - therefore need to ensure compatibility
				* compatibility + easy re-estimation of the performance when the reference set is improved	
			* we do not have the optimum (except for f1)
 			* we estimate it (how: running some algorithms) and it is meant to be changed with time (improved with time)
 			* things are based on the archive of nondominated solutions
 			* we measure the hypervolume difference between the dynamic archive and this reference set.
			* negative hyp-vol diff values are expected (means the algorithm improves over the current reference set)
			* archive is improved over time, whenever we have a new point entering the archive we recompute and log the hyp-vol difference.

			
Dealing with Unknown Optima
---------------------------
The equivalent of a global optimum in the multi-objective case is the set of Pareto-optimal
or efficient solutions, also known as Pareto set. If we assume the search space to be
:math:`\IR^n` and the minimization of two objective
functions :math:`f_1: x\in \IR^n \mapsto f_1(x)\in\IR` and :math:`f_1: x\in \IR^n \mapsto f_1(x)\in\IR`,
a solution :math`:x\in\IR^n` is called Pareto-optimal if it is not dominated
by any other solution :math:`y\in\IR^n` or, in other words, if
:math:`\not\exist y \text{ s.t. } (f_1(y)< f_1(x) \text{ and } f_2(y)\leq f_2(x)) \text{ or } (f_2(y)\leq f_2(x) \text{ and } f_2(y)< f_2(x))`.
The image of the Pareto set under the vector-valued objective function
:math:`f(x)= (f_1(x), f_2(x))` is called Pareto front.

When combining single-objective functions to multi-objective ones as in the case of the ``bbob-biobj``
suite, one cannot expect that Pareto set and Pareto front can be described in analytical form---even
if the single-objective optima are known. Comparing algorithm performance can therefore only be
done relatively to the best known optimum. In the multi-objective
case, where with the Pareto set a set of solutions is sought, we call this approximation
**reference set**. In practice, such a reference set is typically generated by running a certain set
of algorithms on the considered problem ahead of the performance assessment.

This has two main implications:
* Performance can only be judged relatively to the reference set. The better the algorithms
  used to create the reference set have been, the more accurate the performance assessment.
* The reference set is expected to evolve over time, in terms of becoming a better and better
  approximation of the actual Pareto set/Pareto front if more and more algorithms are
  compared.

The performance assessment via the Coco platform addresses both issues, see
`Choice of Reference Set and Target Difficulties`_ and
`Data storage and recalculation of indicator values`_ below for details. Before we discuss
these issues, however, let us have a look on the actual performance criterion used for the
``bbob-biobj`` test suite, assuming that a reference set is given.



Biobjective Performance Assessment in Coco: A Set-Indicator Value Replaces the Objective Function
-------------------------------------------------------------------------------------------------
The general concepts of how the Coco platform suggests to benchmark multi-objective algorithms
is the same than in the single-objective case: for each optimization algorithm, we record the
(expected) runtimes to reach given target precisions for each problem in a given benchmark suite.
A problem thereby consists of a (vector-valued) objective function, its search space dimension,
and a concrete instantiation of it (see [coco-functions-doc]_ ). For defining the target precision
of such a problem, we assume a single-objective criterion which is to be optimized. In the single-objective
case, this is the objective function, in the case of the ``bbob-biobj`` test suite, 
a so-called quality indicator can transform the multi-objective problem into a single-objective
one.

In particular, we suggest to (mainly) use the hypervolume indicator of the archive of all non-dominated
solutions evaluated so far as the quality of an algorithm but principally, other quality indicators
of the archive can be used as well. The reason for proposing to use the quality of the archive is that
in practice 


* why hypervolume (can also be in principle with other indicators)

* Evaluation based on the complete archive of nondominated solutions, independent of population size (Tobias)

* explain - give formula for the computation of the hypervolume (if there are no points dominating the Nadir)



Choice of Reference Set and Target Difficulties
-----------------------------------------------
Choice of the targets based on best estimation of Pareto front (using all the data we have) - chosen instance wise

relative targets (in terms of the hypervolume difference to the hypervolume of the reference set)
are chosen the same for all functions, dimensions, and instances: recorded are 100 targets per order of magnitude,
equi-distantly chosen on the log-scale. Displayed are finally only 10 targets per order of magnitude, in total
51 of them between :math:`10^0` and :math:`10^{-5}`

Note that due to the approximative nature of the reference set and its hypervolume, negative hypervolume values are
possible. The Coco platform stores all

Remind that performance assessment is "relative" because best
estimation of the front is meant to change. Hence ECDF plots are meant
to be reploted.



Data storage and recalculation of indicator values
--------------------------------------------------
We store everything (while not used at the moment in the postprocessing)
but such that we can recompute things if needed. All this will
be used to recompute the reference set.



Instances and Generalization Experiment
---------------------------------------
* we record for 10 instances but display result for only 5. This will allow us to generate data for an unbiased
  generalization test on the unseen instances


Contents:

.. toctree::
   :maxdepth: 2



   
.. [bbob-biobj-functions-doc] The BBOBies. **Function Documentation of the bbob-biobj Test Suite**. http://numbbo.github.io/coco-doc/bbob-biobj/functions/

.. [coco-functions-doc] The BBOBies. **COCO: Performance Assessment**. http://numbbo.github.io/coco-doc/perf-assessment/

.. [coco-doc] The BBOBies. **COCO: A platform for Comparing Continuous Optimizers in a Black-Box Setting**. http://numbbo.github.io/coco-doc/