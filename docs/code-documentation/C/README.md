The COCO/NumBBO experiments interface
=====================================

<a href="https://github.com/numbbo/coco">COCO (COmparing Continuous Optimisers)</a> is a platform 
for systematic and sound comparisons of real-parameter global optimisers mainly developed within the 
<a href="http://numbbo.gforge.inria.fr/doku.php">NumBBO project</a>. COCO provides benchmark function 
testbeds, experimentation templates which are easy to parallelize, and tools for processing and 
visualizing data generated by one or several optimizers.

For a getting started guide see  
[here](https://github.com/numbbo/coco/blob/master/README.md#getting-started). 

### Reimplementation of COCO in ANSI C

In order to allow for easier maintenance and further extensions of the COCO platform, it was rewritten
entirely from 2014 till 2016. Now, a single implementation in ANSI C (aka C89) is used and called from
the other languages to conduct the experiments. This documentation of the COCO C code serves therefore
as the basic reference for:
 - How to conduct benchmarking experiments in C
 - How to write new test functions and combine them into test suites
 - How to write additional performance indicators and logging functionality

### Pointers to the source code and other documentation

The actual source code of COCO can be found at http://github.com/numbbo/coco

More information about the biobjective test suite (bbob-biobj) can be found at
http://numbbo.github.io/bbob-biobj-functions-doc

How to conduct experiments in all supported languages is described at
http://numbbo.github.io/bbob-biobj-experiments-doc (coming soon)

- - - - - - - - - - - - - - - - - - - - -

## How to conduct benchmarking experiments in C 

At this point we assume that the ``example_experiment`` in C is running on your machine (see the 
[getting started guide](https://github.com/numbbo/coco/blob/master/README.md#getting-started) if you 
need any assistance). The best way to create your own benchmark experiment is to copy that example and
make the changes you need to include your optimizer. 

Benchmarking the algorithm ``my_optimizer`` on the ``bbob-biobj`` suite with default parameters is  
invoked in the following way:

    coco_suite_t *suite;
    coco_observer_t *observer;
    coco_problem_t *problem;

    suite = coco_suite("bbob-biobj", "", "");
    observer = coco_observer("bbob-biobj", "");

    while ((problem = coco_suite_get_next_problem(suite, observer)) != NULL) {
      my_optimizer(problem);
    }

    coco_observer_free(observer);
    coco_suite_free(suite);

The ``coco_suite_t`` object is a collection of (in this case biobjective) optimization problems of 
type ``coco_problem_t``. Using the while loop, we iterate through all problems of the suite and 
optimize each of them with ``my_optimizer`` (a simple random search is used in the 
``example_experiment``). The ``coco_observer_t`` object takes care of logging the performance of the 
optimizer. Note that the benchmarking procedure remains the same whether we are dealing with single- 
or multi-objective problems and algorithms. To perform benchmarking on a different suite and with a 
different observer, just replace ``"bbob-biobj"`` with the name of the desired suite and observer. 

__Missing explanation of the algorithm BUDGET (number of evaluations)! Do we have any 
guidelines on how to set it?__

In the above example, the suite and observer are called without additional parameters (the empty 
strings ``""`` are used), which means that the default values apply. These can be changed by 
calling:

    suite = coco_suite("bbob-biobj", suite_instance, suite_options);
    observer = coco_observer("bbob-biobj", observer_options);

where ``suite_instance``, ``suite_options`` and ``observer_options`` are strings with parameters 
encoded  as pairs ``"key: value"``. When the value is a range of integers, it can be encoded using 
the syntax ``m-n`` (meaning all integer values from m to n), ``-n`` (meaning all values up to n), 
``n-`` (meaning all values from n on) and even ``-`` (meaning all available values); or by simply 
listing the values separated by commas (as in ``2,3,5``). No spaces are allowed in the definition of 
a range or list of values. 

### Suite parameters

The suite contains a collection of problems constructed by a Cartesian product of the suite's 
optimization  functions, dimensions and instances. The functions and dimensions are defined by the 
suite name, while the instances are defined with the ``suite_instance`` parameter. The suite can be 
thinned by filtering of  functions, dimensions and instances through the ``suite_options`` parameter. 

Possible keys and values for ``suite_instance`` are:
- either ``"year: YEAR"``, where ``YEAR`` is usually the year of the corresponding [BBOB 
workshop](http://numbbo.github.io/workshops) defining the instances used in that year's benchmark,
- or ``"instances: RANGE"``, where ``RANGE`` is a range of instances you wish to include in the suite 
(they start from 1 and continue indefinitely; see above for range syntax).

If both ``year`` and ``instances`` appear in the ``suite_instance`` string, only the first one is 
taken into account. If no ``suite_instance`` is given, it defaults to the year of the current BBOB 
workshop. 

Possible keys and values for ``suite_options`` are:
- ``dimensions: LIST``, where ``LIST`` is the list of dimensions to keep in the suite, 
- ``function_idx: RANGE``, where ``RANGE`` is a range or list of function indexes (starting from 1) to 
keep in the suite, and
- ``instance_idx: RANGE``, where ``RANGE`` is a range or list of instance indexes (starting from 1) to 
keep in the suite. 

For example, the call:

    suite = coco_suite("bbob-biobj", 
                       "instances: 10-20", 
                       "dimensions: 2,3,5,10,20 instance_idx:1-5");

first creates the biobjective suite with instances 10 to 20, but then uses only the first five 
dimensions (skipping dimension 40) and the first five instances (i.e. instances 10 to 14) of the suite. 

See [biobjective test suite](http://numbbo.github.io/bbob-biobj-functions-doc) and 
[bbob test sute](http://numbbo.github.io/bbob-functions-doc) for more detailed information on the two 
currently supported suites.

### Observer parameters

The observer controls the logging that is performed within the benchmark. Some observer parameters are 
general, while others are specific to the chosen observer. 

Possible keys and values for the general ``observer_options`` are:
- ``result_folder: NAME``, determines the output folder. If the folder with the given name already 
exists, ``NAME_001`` will be tried and so on. The default value is "results".
- ``algorithm_name: NAME``, where ``NAME`` is a short name of the algorithm that will be used in plots 
(no spaces are allowed). The default value is "ALG".
- ``algorithm_info: STRING`` stores the description of the algorithm. If it contains spaces, it must be 
surrounded by double quotes. The default value is "" (no description).
- ``log_level: STRING`` denotes the level of information given to the user through the standard output 
and error stream messages (this is not connected with the logging in the files). ``STRING`` can take 
on the values ``error`` (only error messages are output), ``warning`` (only error and warning messages 
are output),  ``info`` (only error, warning and info messages are output) and ``debug`` (all messages 
are output). The default value is info. 
- ``precision_x: VALUE`` defines the precision used when outputting variables and corresponds to the 
number of digits to be printed after the decimal point. The default value is 8.
- ``precision_f: VALUE`` defines the precision used when outputting f values and corresponds to the 
number of digits to be printed after the decimal point. The default value is 15.

Possible keys and values for the ``observer_options`` of the ``bbob-biobj`` observer are:
- TODO

## How to write new test functions and combine them into test suites

TODO

## How to write additional performance indicators and logging functionality

TODO
