% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,12pt,english]{article}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}
\usepackage{amsfonts}

\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }


  \usepackage{amssymb}
  \pagestyle{plain}
  \newcommand{\chapter}[1]{}  % hack to be able to use article documentclass
  \newcommand{\ignore}[1]{}  % never used
  \newcommand{\COCO}{\href{https://githum.com/numbbo/coco}{COCO}}

  \newcommand{\R}{\ensuremath{\mathbb{R}}}
  \newcommand{\ve}[1]{{\boldsymbol{#1}}}
  \newcommand{\x}{\ensuremath{\ve{x}}}
  \newcommand{\finstance}{\ensuremath{f^j}}


\title{{COCO}: {A} Platform for Comparing Continuous Optimizers in a Black-Box Setting}
\date{March 27, 2016}
\release{0.9}
\author{Nikolaus Hansen$^{1,2}$, 
      Anne Auger$^{1,2}$, 
      Olaf Mersmann$^3$, 
      Tea Tusar$^4$, 
      Dimo Brockhoff$^4$
  \\
    $^1$Inria, research centre Saclay, France
  \\
   $^2$Universit\'e Paris-Saclay, LRI, France
  \\
    $^3$TU Dortmund University, Chair of Computational Statistics, Germany
  \\
    $^4$Inria, research centre Lille, France
    }
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}

\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
%\tableofcontents
\phantomsection\label{index::doc}



\chapter{CHAPTERTITLE}
\label{index:coco-a-platform-for-comparing-continuous-optimizers-in-a-black-box-setting}\label{index:chaptertitle}% %\tableofcontents is automatic with sphinx and moved behind abstract by swap...py
\begin{abstract}
\href{https://github.com/numbbo/coco}{COCO} is a platform for Comparing Continuous Optimizers in a black-box
setting.
It aims at automatizing the tedious and repetitive task of
benchmarking numerical optimization algorithms to the greatest possible
extent.
We present the rationals behind the development of the platform
as a general proposition for a guideline towards better benchmarking.
We detail underlying fundamental concepts of
\href{https://github.com/numbbo/coco}{COCO} such as its definition of
a problem, the idea of instances, the relevance of target values, and runtime
as central performance measure.
Finally, we  give a quick overview of the basic
code structure and the available test suites.
\end{abstract}\tableofcontents
\newpage

\section{Introduction}
\label{index:introduction}
We consider the continuous black-box optimization or search problem to minimize
\begin{gather}
\begin{split}f: X\subset\mathbb{R}^n \to \mathbb{R}^m \qquad n,m\ge1\end{split}\notag
\end{gather}
such that for the \(l\) constraints
\begin{gather}
\begin{split}g: X\subset\mathbb{R}^n \to \mathbb{R}^l \qquad l\ge0\end{split}\notag
\end{gather}
we have \(g_i(\x)\le0\) for all \(i=1\dots l\).
More specifically, we aim to find, as quickly as possible, one or several solutions \(\x\) in the search space \(X\) with \emph{small} value(s) of \(f(\x)\in\mathbb{R}^m\) that satisfy all above constraints \(g\).
We consider \emph{time} to be defined as the number of calls to the function \(f\).

A continuous optimization algorithm, also known as \emph{solver}, addresses the
above problem.
Here, we assume that \(X\) is known, but no prior knowledge about \(f\) or
\(g\) is available to the algorithm.
That is, \(f\) and \(g\) are considered as a black-box which the algorithm can
query with solutions \(\x\in\mathbb{R}^n\) to get the respective values
\(f(\x)\) and \(g(\x)\).

From these prerequisits, benchmarking optimization algorithms seems to be a
rather simple and straightforward task. We run an algorithm on a collection of
problems and display the results. However, under closer inspection,
benchmarking turns out to be surprisingly tedious, and it appears to be
difficult to get results that can be meaningfully interpreted beyond the
standard claim that one algorithm is better than another on some problems. \footnote[1]{
One major flaw is that we often get no
indication of \emph{how much} better an algorithm is.
That is, the results of benchmarking often provide no indication of
\emph{relevance};
the main output often consists of hundreds of tabulated numbers
only interpretable on an \emph{ordinal scale} \phantomsection\label{index:id7}{\hyperref[index:ste1946]{\emph{{[}STE1946{]}}}}. Addressing a point of a
common confusion, \emph{statistical significance} is only a secondary, and by no
means a \emph{sufficient} condition for \emph{relevance}.
}
Here, we offer a conceptual guideline for benchmarking
continuous optimization algorithms which tries to address this challenge and
has been implemented within the \href{https://github.com/numbbo/coco}{COCO} framework. \footnote[2]{
See \href{https://www.github.com/numbbo/coco}{https://www.github.com/numbbo/coco} or \href{https://numbbo.github.io}{https://numbbo.github.io} for implementation details.
}

The \href{https://github.com/numbbo/coco}{COCO} framework provides the practical means for an automatized
benchmarking procedure. Installing \href{https://github.com/numbbo/coco}{COCO} (in a shell) and benchmarking an
optimization algorithm, say, implemented in the function \code{fmin} in Python,
becomes as simple as
in Figure 1. \begin{figure} %\begin{minipage}{\textwidth}
\begin{Verbatim}[commandchars=\\\{\}]
\PYGZdl{} git clone https://github.com/numbbo/coco.git  \PYGZsh{} get coco
\PYGZdl{} cd coco
\PYGZdl{} python do.py run\PYGZhy{}python  \PYGZsh{} install Python experimental module cocoex
\PYGZdl{} python do.py install\PYGZhy{}postprocessing  \PYGZsh{} install post\PYGZhy{}processing :\PYGZhy{})
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+ch}{\PYGZsh{}!/usr/bin/env python}
\PYG{k+kn}{import} \PYG{n+nn}{cocoex}
\PYG{k+kn}{import} \PYG{n+nn}{cocopp}  \PYG{c+c1}{\PYGZsh{} or: import bbob\PYGZus{}pproc as cocopp}
\PYG{k+kn}{from} \PYG{n+nn}{myoptimizer} \PYG{k+kn}{import} \PYG{n}{fmin}

\PYG{n}{suite} \PYG{o}{=} \PYG{n}{cocoex}\PYG{o}{.}\PYG{n}{Suite}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bbob}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year: 2016}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{observer} \PYG{o}{=} \PYG{n}{cocoex}\PYG{o}{.}\PYG{n}{Observer}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bbob}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{result\PYGZus{}folder: myoptimizer\PYGZhy{}on\PYGZhy{}bbob}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{suite}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} loop over all problems}
    \PYG{n}{observer}\PYG{o}{.}\PYG{n}{observe}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} prepare logging of necessary data}
    \PYG{n}{fmin}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{n}{p}\PYG{o}{.}\PYG{n}{initial\PYGZus{}solution}\PYG{p}{)}

\PYG{n}{cocopp}\PYG{o}{.}\PYG{n}{main}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{exdata/myoptimizer\PYGZhy{}on\PYGZhy{}bbob}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} invoke data post\PYGZhy{}processing}
\end{Verbatim}
\caption[Minimal benchmarking code in Python]{
Shell code for installation of \COCO\ (above), and Python code to benchmark
\texttt{fmin} on the \texttt{bbob} suite and display the results.
Now the file \code{ppdata/ppdata.html} can be used to browse the resulting data.
}
\end{figure}
The \href{https://github.com/numbbo/coco}{COCO} framework provides currently
\begin{itemize}
\item {} 
an interface to several languages in which the benchmarked optimizer
can be written, currently C/C++, Java, Matlab/Octave, Python

\item {} 
several benchmark suites or testbeds, currently all written in C

\item {} 
data logging facilities via the \code{Observer}

\item {} 
data post-processing in Python and data display facilities in \code{html}

\item {} 
article LaTeX templates

\end{itemize}

The underlying philosophy of \href{https://github.com/numbbo/coco}{COCO} is to provide everything which otherwise
most experimenters needed to setup and implement themselves, if they wanted to
benchmark an algorithm properly. So far, the framework has been used successfully for
benchmarking far over a hundred algorithms by many researchers.


\subsection{Why COCO?}
\label{index:why-coco}
Appart from diminishing the burden (time) and the pitfalls (and bugs
or omissions) of the repetitive coding task by many experimenters, our aim is to
provide a \emph{conceptual guideline for better benchmarking}. Our guideline has
the following defining features.
\begin{enumerate}
\item {} \begin{description}
\item[{Benchmark functions are}] \leavevmode\begin{enumerate}
\item {} 
used as black boxes for the algorithm, however they
are explicitly known to the scientific community.

\item {} 
designed to be comprehensible, to allow a meaningful
interpretation of performance results.

\item {} 
difficult to ``defeat'', that is, they do not
have artificial regularities that can be (intentionally or unintentionally)
exploited by an algorithm. \footnote[3]{
For example, the optimum is not in all-zeros, optima are not placed
on a regular grid, most functions are not separable \phantomsection\label{index:id17}{\hyperref[index:whi1996]{\emph{{[}WHI1996{]}}}}. The
objective to remain comprehensible makes it more challenging to design
non-regular functions. Which regularities are common place in real-world
optimization problems remains an open question.
}

\item {} 
scalable with the input dimension \phantomsection\label{index:id10}{\hyperref[index:whi1996]{\emph{{[}WHI1996{]}}}}.

\end{enumerate}

\end{description}

\item {} 
There is no predefined budget (number of \(f\)-evaluations) for running an
experiment, the experimental procedure is \emph{budget-free} \phantomsection\label{index:id11}{\hyperref[index:han2016ex]{\emph{{[}HAN2016ex{]}}}}.

\item {} 
A single performance measure is used --- and thereafter aggregated and
displayed in
several ways --- namely \textbf{runtime}, \emph{measured in
number of} \(f\)-\emph{evaluations} \phantomsection\label{index:id12}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}}. Runtime has the advantages to
\begin{itemize}
\item {} 
be independent of the computational platform, language, compiler, coding
styles, and other specific experimental conditions \footnote[4]{
Runtimes measured in \(f\)-evaluations are widely
comparable and designed to stay. The experimental procedure
\phantomsection\label{index:id19}{\hyperref[index:han2016ex]{\emph{{[}HAN2016ex{]}}}} includes however a timing experiment which records the
internal computational effort of the algorithm in CPU or wall clock time.
}

\item {} 
be relevant, meaningful and easily interpretable without expert domain knowledge

\item {} 
be quantitative on the ratio scale \phantomsection\label{index:id14}{\hyperref[index:ste1946]{\emph{{[}STE1946{]}}}} \footnote[5]{
As opposed to a ranking of algorithm based on their solution quality
achieved after a given budget.
}

\item {} 
assume a wide range of values

\item {} 
aggregate over a collection of values in a meaningful way

\end{itemize}

A \emph{missing} runtime value is considered as possible outcome (see below).

\item {} 
The display is as comprehensible, intuitive and informative as possible,
We believe that details matter.
Aggregation over dimension is avoided, because dimension is an a priori
known parameter that can and should be used for algorithm design or selection
decisions.

\end{enumerate}


\subsection{Terminology}
\label{index:terminology}
We specify a few terms which are used later.
\begin{description}
\item[{\emph{function}}] \leavevmode
We talk about a \emph{function} as a parametrized mapping
\(\mathbb{R}^n\to\mathbb{R}^m\) with scalable input space, and usually \(m\in\{1,2\}\).
Functions are parametrized such that different \emph{instances} of the
``same'' function are available, e.g. translated or shifted versions.

\item[{\emph{problem}}] \leavevmode
We talk about a \emph{problem}, \href{http://numbbo.github.io/coco-doc/C/coco\_8h.html\#a408ba01b98c78bf5be3df36562d99478}{\code{coco\_problem\_t}}, as a specific \emph{function
instance} on which an optimization algorithm is run.
A problem
can be evaluated and returns an \(f\)-value or -vector and, in case,
a \(g\)-vector.
In the context of performance assessment, a target \(f\)- or
indicator-value is added to define a problem.

\item[{\emph{runtime}}] \leavevmode
We define \emph{runtime}, or \emph{run-length} \phantomsection\label{index:id21}{\hyperref[index:hoo1998]{\emph{{[}HOO1998{]}}}} as the \emph{number of
evaluations} conducted on a given problem until a prescribed target value is
hit, also referred to as number of \emph{function} evaluations or \(f\)-evaluations.
Runtime is our central performance measure.

\item[{\emph{suite}}] \leavevmode
A test- or benchmark-suite is a collection of problems, typically between
twenty and a hundred, where the number of objectives \(m\) is fixed.

\end{description}


\section{Functions, Instances, Problems, and Targets}
\label{index:functions-instances-problems-and-targets}
In the \href{https://github.com/numbbo/coco}{COCO} framework we consider \textbf{functions}, \(f_i\), for each suite
distinguished by their identifier \(i=1,2,\dots\) .
Functions are further
\emph{parametrized} by the (input) dimension, \(n\), and the instance number, \(j\), \footnote[6]{
We can think of \(j\) as a continuous parameter vector, as it
parametrizes, among others things, translations and rotations. In practice,
\(j\) is a discrete identifier for single instantiations of these parameters.
}
that is, for a given \(m\) we have
\begin{gather}
\begin{split}\finstance_i \equiv f(n, i, j):\R^n \to \mathbb{R}^m \quad
\x \mapsto \finstance_i (\x) = f(n, i, j)(\x)\enspace.\end{split}\notag
\end{gather}
Varying \(n\) or \(j\) leads to a variation of the same function
\(i\) of a given suite.
By fixing \(n\) and \(j\) for function \(f_i\), we define an optimization \textbf{problem}
\((n, i, j)\equiv(f_i, n, j)\) that can be presented to the optimization algorithm. Each problem receives again
an index in the suite, mapping the triple \((n, i, j)\) to a single
number.

As the formalization above suggests, the differentiation between function (index)
and instance index is of purely semantic nature.
This semantics however is important in how we display and
interpret the results. We interpret \textbf{varying the instance} parameter as
a natural randomization for experiments \footnote[7]{
Changing or sweeping through a relevant feature of the problem class,
systematically or randomized, is another possible usage of instance
parametrization.
} in order to
\begin{itemize}
\item {} 
generate repetitions on a function and

\item {} \begin{description}
\item[{average away irrelevant aspects of a function thereby providing}] \leavevmode\begin{itemize}
\item {} 
generality which alleviates the problem of overfitting, and

\item {} 
a fair setup which prevents intentional or unintentional exploitation of
irrelevant or artificial function properties.

\end{itemize}

\end{description}

\end{itemize}

For example, we consider the absolute location of the optimum not a defining
function feature. Consequently, in a typical \href{https://github.com/numbbo/coco}{COCO} benchmark suite, instances
with randomized search space translations are presented to the optimizer. \footnote[8]{
Conducting either several trials on instances with randomized search space
translations or with a randomized initial solution is equivalent, given
that the optimizer behaves translation invariant (disregarding domain
boundaries).
}


\section{Runtime and Target Values}
\label{index:runtime-and-target-values}
In order to measure the runtime of an algorithm on a problem, we
establish a hitting time condition.
We prescribe a \textbf{target value}, \(t\), which is an \(f\)- or
indicator-value \phantomsection\label{index:id28}{\hyperref[index:tus2016]{\emph{{[}TUS2016{]}}}}.
For a single run, when an algorithm reaches or surpasses the target value \(t\)
on problem \((f_i, n, j)\), we say it has \emph{solved the problem} \((f_i, n, j, t)\) --- it was successful. \footnote[9]{
Note the use of the term \emph{problem} in two meanings: as the problem the
algorithm is benchmarked on, \((f_i, n, j)\), and as the problem, \((f_i, n, j, t)\), an algorithm can
solve by hitting the target \(t\) with the runtime, \(\mathrm{RT}(f_i, n, j, t)\), or may fail to solve.
Each problem \((f_i, n, j)\) gives raise to a collection of dependent problems \((f_i, n, j, t)\).
Viewed as random variables, the events \(\mathrm{RT}(f_i, n, j, t)\) given \((f_i, n, j)\) are not
independent events for different values of \(t\).
}

Now, the \textbf{runtime} is the evaluation count when the target value \(t\) was
reached or surpassed for the first time.
That is, runtime is the number of \(f\)-evaluations needed to solve the problem
\((f_i, n, j, t)\) (but see also \href{https://www.github.com}{Recommendations} in \phantomsection\label{index:id30}{\hyperref[index:han2016ex]{\emph{{[}HAN2016ex{]}}}}). \footnote[10]{
Target values are directly linked to a problem, leaving the burden to
properly define the targets with the designer of the benchmark suite.
The alternative is to present final \(f\)- or indicator-values as results,
leaving the (rather unsurmountable) burden to interpret these values to the
reader.
Fortunately, there is an automatized generic way to generate target values
from observed runtimes, the so-called run-length based target values
\phantomsection\label{index:id35}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}}.
}
\emph{Measured runtimes are the only way of how we assess the performance of an
algorithm.} \footnote[11]{
Observed success rates can (and should) be translated into lower bounds
on runtimes on a subset of problems.
}

If an algorithm does not hit the target in a single run, the runtime remains
undefined --- while
it has been bound to be at least \(k+1\), where \(k\) is the number of
evaluations in this unsuccessful run.
The number of defined runtime values depends on the budget the
algorithm has explored.
Therefore, larger budgets are preferable --- however they should not come at
the expense of abandoning reasonable termination conditions. Instead,
restarts should be done.


\subsection{Restarts and Simulated Restarts}
\label{index:restarts-and-simulated-restarts}\label{index:sec-restarts}
An optimization algorithm is bound to terminate and, in the single-objective case, return a recommended
solution, \(\x\), for the problem, \((f_i, n, j)\).
It solves thereby all problems \((f_i, n, j, t)\) for which \(f(\x)\le t\).
Independent restarts from different, randomized initial solutions are a simple
but powerful tool to increase the number of solved problems \phantomsection\label{index:id37}{\hyperref[index:har1999]{\emph{{[}HAR1999{]}}}} --- namely by increasing the number of \(t\)-values, for which the problem \((f_i, n, j)\)
was solved. \footnote[12]{
For a given problem \((f_i, n, j)\), the number of acquired runtime values, \(\mathrm{RT}(f_i, n, j, t)\)
is monotonously increasing with the budget used. Considered as random
variables, these runtimes are not independent.
}
Independent restarts tend to increase the success rate, but they generally do
not \emph{change} the performance \emph{assessment}, because the successes materialize at
greater runtimes.
Therefore, we call our approach \emph{budget-free}.
Restarts however ``\emph{improve the reliability, comparability, precision, and ``visibility'' of the measured results}'' \phantomsection\label{index:id39}{\hyperref[index:han2016ex]{\emph{{[}HAN2016ex{]}}}}.

\emph{Simulated restarts} \phantomsection\label{index:id40}{\hyperref[index:han2010ex]{\emph{{[}HAN2010ex{]}}}} \phantomsection\label{index:id41}{\hyperref[index:han2010]{\emph{{[}HAN2010{]}}}} \phantomsection\label{index:id42}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}} are used to determine a runtime for unsuccessful runs. Semantically, this is only valid if we interpret different
instances as random repetitions.
Resembling the bootstrapping method \phantomsection\label{index:id43}{\hyperref[index:efr1993]{\emph{{[}EFR1993{]}}}}, when we face an unsolved problem, we draw uniformly at random a
new \(j\) until we find an instance such that \((f_i, n, j, t)\) was solved. \footnote[13]{
More specifically, we consider the problems \((f_i, n, j, t(j))\) for
all benchmarked instances \(j\). The targets \(t(j)\) depend on the instance
in a way to make the problems comparable \phantomsection\label{index:id47}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}}.
}
The evaluations done on the first unsolved problem and on all subsequently
drawn unsolved problems are added to the runtime on the last problem and
are considered as runtime on the original unsolved problem.
This method is applied if a problem instance was not solved and is
(only) available if at least one problem instance was solved.


\subsection{Aggregation}
\label{index:aggregation}
A typical benchmark suite consists of about 20--100 functions with 5--15 instances for each function. For each instance, up to about 100 targets are considered for the
performance assessment. This means we want to consider at least \(20\times5=100\), and
up to \(100\times15\times100=150\,000\) runtimes for the performance assessment.
To make them amenable to the experimenter, we need to summarize these data.

Our idea behind an aggregation is to make a statistical summary over a set or
subset of \emph{problems of interest} over which we assume a uniform distribution \phantomsection\label{index:id48}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}}.
From a practical perspective this means to have no simple way to distinguish
between these problems and to select an optimization algorithm accordingly---in
which case an aggregation would have no significance---and that we are likely
to face each problem with similar probability.
We do not aggregate over dimension, because dimension can and
should be used for algorithm selection.

We have several ways to aggregate the resulting runtimes.
\begin{itemize}
\item {} 
Empirical cumulative distribution functions (ECDF). In the domain of
optimization, ECDF are also known as \emph{data profiles} \phantomsection\label{index:id49}{\hyperref[index:mor2009]{\emph{{[}MOR2009{]}}}}. We
prefer the simple ECDF over the more innovative performance profiles
\phantomsection\label{index:id50}{\hyperref[index:mor2002]{\emph{{[}MOR2002{]}}}} for two reasons.
ECDF (i) do not depend on other presented algorithms, that is, they are
entirely comparable across different publications, and (ii) let us distinguish in a
natural way easy problems from difficult problems for the considered
algorithm. We usually display ECDF on the log scale, which makes the area
above the curve and the \emph{difference area} between two curves a meaningful
conception \phantomsection\label{index:id51}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}}.

\item {} 
Averaging, as an estimator of the expected runtime. The average runtime, that
is the estimated expected runtime, is
often plotted against dimension to indicate scaling with dimension. The
\emph{arithmetic} average
is only meaningful if the underlying distribution of the values
is similar. Otherwise, the average of log-runtimes, or \emph{geometric} average,
is useful.

\item {} 
Restarts and simulated restarts, see Section {\hyperref[index:sec\string-restarts]{\emph{Restarts and Simulated Restarts}}}, do not
literally aggregate runtimes (which are literally defined only when \(t\) was
hit).  They aggregate, however, time data to eventually supplement missing runtime
values, see also \phantomsection\label{index:id52}{\hyperref[index:bbo2016perf]{\emph{{[}BBO2016perf{]}}}}.

\end{itemize}


\section{General Code Structure}
\label{index:general-code-structure}
The code basis of the \href{https://github.com/numbbo/coco}{COCO} code consists of two parts.
\begin{description}
\item[{The \emph{Experiments} part}] \leavevmode
defines test suites, allows to conduct experiments, and provides the output
data. The \href{http://numbbo.github.io/coco-doc/C}{code base is written in C}, and wrapped in different languages
(currently Java, Python, Matlab/Octave). An amalgamation technique is used
that outputs two files \code{coco.h} and \code{coco.c} which suffice to run
experiments within the \href{https://github.com/numbbo/coco}{COCO} framework.

\item[{The \emph{post-processing} part}] \leavevmode
processes the data and displays the resulting runtimes. This part is
entirely written in Python and heavily depends on \href{http://matplotlib.org/}{\code{matplotlib}} \phantomsection\label{index:id54}{\hyperref[index:hun2007]{\emph{{[}HUN2007{]}}}}.

\end{description}


\section{Test Suites}
\label{index:test-suites}\label{index:matplotlib}
Currently, the \href{https://github.com/numbbo/coco}{COCO} framework provides three different test suites.
\begin{description}
\item[{\code{bbob}}] \leavevmode
contains 24 functions in five subgroups \phantomsection\label{index:id55}{\hyperref[index:han2009fun]{\emph{{[}HAN2009fun{]}}}}.

\item[{\code{bbob-noisy}}] \leavevmode
contains 30 noisy problems in three subgroups \phantomsection\label{index:id56}{\hyperref[index:han2009noi]{\emph{{[}HAN2009noi{]}}}},
currently only implemented in the \href{http://coco.gforge.inria.fr/doku.php?id=downloads}{old code basis}.

\item[{\code{bbob-biobj}}] \leavevmode
contains 55 bi-objective (\(m=2\)) functions in 15 subgroups \phantomsection\label{index:id57}{\hyperref[index:tus2016]{\emph{{[}TUS2016{]}}}}.

\end{description}
\section*{Acknowledgments}
The authors would like to thank Raymond Ros, Steffen Finck, Marc Schoenauer,
Petr Posik and Dejan Tusar for their many invaluable contributions to this work.

The authors also acknowledge support by the grant ANR-12-MONU-0009 (NumBBO)
of the French National Research Agency.

\begin{thebibliography}{BBO2016perf}
\bibitem[BBO2016perf]{BBO2016perf}{\phantomsection\label{index:bbo2016perf} 
The BBOBies (2016). \href{http://numbbo.github.io/coco-doc/perf-assessment}{COCO: Performance Assessment}.
}
\bibitem[HAN2010ex]{HAN2010ex}{\phantomsection\label{index:han2010ex} 
N. Hansen, A. Auger, S. Finck, and R. Ros (2010).
Real-Parameter Black-Box Optimization Benchmarking 2010: Experimental Setup, \emph{Inria Research Report} RR-7215 \href{http://hal.inria.fr/inria-00362649/en}{http://hal.inria.fr/inria-00362649/en}
}
\bibitem[HAN2010]{HAN2010}{\phantomsection\label{index:han2010} 
N. Hansen, A. Auger, R. Ros, S. Finck, and P. Posik (2010).
Comparing Results of 31 Algorithms from the Black-Box Optimization Benchmarking BBOB-2009. Workshop Proceedings of the GECCO Genetic and Evolutionary Computation Conference 2010, ACM, pp. 1689-1696
}
\bibitem[HAN2009fun]{HAN2009fun}{\phantomsection\label{index:han2009fun} 
N. Hansen, S. Finck, R. Ros, and A. Auger (2009).
\href{http://coco.gforge.inria.fr/}{Real-parameter black-box optimization benchmarking 2009: Noiseless functions definitions}. \href{https://hal.inria.fr/inria-00362633}{Technical Report RR-6829}, Inria, updated February 2010.
}
\bibitem[HAN2009noi]{HAN2009noi}{\phantomsection\label{index:han2009noi} 
N. Hansen, S. Finck, R. Ros, and A. Auger (2009).
\href{http://coco.gforge.inria.fr/}{Real-Parameter Black-Box Optimization Benchmarking 2009: Noisy Functions Definitions}. \href{https://hal.inria.fr/inria-00369466}{Technical Report RR-6869}, Inria, updated February 2010.
}
\bibitem[HAN2016ex]{HAN2016ex}{\phantomsection\label{index:han2016ex} 
N. Hansen, T. Tusar, A. Auger, D. Brockhoff, O. Mersmann (2016).
\href{http://numbbo.github.io/coco-doc/experimental-setup/}{COCO: Experimental Procedure}.
}
\bibitem[HUN2007]{HUN2007}{\phantomsection\label{index:hun2007} 
J. D. Hunter (2007). \href{http://matplotlib.org/}{Matplotlib}: A 2D graphics environment,
\emph{Computing In Science \& Engineering}, 9(3): 90-95.
}
\bibitem[EFR1993]{EFR1993}{\phantomsection\label{index:efr1993} 
B. Efron and R. Tibshirani (1993). An introduction to the
bootstrap. Chapman \& Hall/CRC.
}
\bibitem[HAR1999]{HAR1999}{\phantomsection\label{index:har1999} 
G. R. Harik and F. G. Lobo (1999). A parameter-less genetic
algorithm. In \emph{Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO)}, volume 1, pages 258-265. ACM.
}
\bibitem[HOO1998]{HOO1998}{\phantomsection\label{index:hoo1998} 
H. H. Hoos and T. Stützle (1998). Evaluating Las Vegas
algorithms: pitfalls and remedies. In \emph{Proceedings of the Fourteenth
Conference on Uncertainty in Artificial Intelligence (UAI-98)},
pages 238-245.
}
\bibitem[MOR2009]{MOR2009}{\phantomsection\label{index:mor2009} 
J. Moré and S. Wild (2009).
Benchmarking Derivative-Free Optimization Algorithms. \emph{SIAM J. Optimization}, 20(1):172-191.
}
\bibitem[MOR2002]{MOR2002}{\phantomsection\label{index:mor2002} 
D. Dolan and J. J. Moré (2002).
Benchmarking Optimization Software with Performance Profiles. \emph{Mathematical Programming}, 91:201-213.
}
\bibitem[STE1946]{STE1946}{\phantomsection\label{index:ste1946} 
S.S. Stevens (1946).
On the theory of scales of measurement. \emph{Science} 103(2684), pp. 677-680.
}
\bibitem[TUS2016]{TUS2016}{\phantomsection\label{index:tus2016} 
T. Tusar, D. Brockhoff, N. Hansen, A. Auger (2016).
\href{http://numbbo.github.io/coco-doc/bbob-biobj/functions/}{COCO: The Bi-objective Black Box Optimization Benchmarking (bbob-biobj)
Test Suite}.
}
\bibitem[WHI1996]{WHI1996}{\phantomsection\label{index:whi1996} 
D. Whitley, S. Rana, J. Dzubera, K. E. Mathias (1996).
Evaluating evolutionary algorithms. \emph{Artificial intelligence}, 85(1), 245-276.
}
\end{thebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
